{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYkvuEthhJFQ"
   },
   "source": [
    "# Bidirectional Feedback Collaborative Network (BFC-Net)\n",
    "\n",
    "Paper: **Â«Ø´Ø¨Ú©Ù‡ Ù‡Ù…Ú©Ø§Ø±ÛŒ Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯ Ø¯ÙˆØ³ÙˆÛŒÙ‡ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ø´ÛŒØ§Ø¡ Ø¨Ø±Ø¬Ø³ØªÙ‡ Ø¯Ø± ØªØµØ§ÙˆÛŒØ± Ù…Ø§Ù‡ÙˆØ§Ø±Ù‡â€ŒØ§ÛŒ Ù†ÙˆØ±ÛŒÂ»**  \n",
    "Train on **ORSSD / EORSSD / ORSI-4199** with shared images, separate GTs.\n",
    "\n",
    "âœ… Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù‚Ø§Ù„Ù‡:\n",
    "- BackBone: ResNet-50 (ImageNet)\n",
    "- Input Size: 352Ã—352\n",
    "- Optimizer: Adam (lr=1e-4)\n",
    "- Batch Size: 8\n",
    "- Epochs: 100\n",
    "- Data Aug: Flip, Rotate, Color Jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:28.934882Z",
     "start_time": "2025-12-18T11:52:28.497140900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Dn_ipcvhQcJ",
    "outputId": "43992548-14b3-40db-8443-41708d449091"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:29.184674700Z",
     "start_time": "2025-12-18T11:52:29.028328800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvTXBYmVhJFT",
    "outputId": "33eda3b8-ec97-44ca-ad25-636df8a05e6e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:29.372226700Z",
     "start_time": "2025-12-18T11:52:29.216009400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mv_rJn3hJFV",
    "outputId": "73907c38-0295-40f5-a7b7-ed6fbeb872fc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk2WxzVUhJFW"
   },
   "source": [
    "### ğŸ§  Cell 2: Define BFC-Net Architecture (from bfc_net.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:29.559993Z",
     "start_time": "2025-12-18T11:52:29.388317300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0YWiHRDhJFW",
    "outputId": "f6b40759-b49e-4b2d-de9f-6a47f1fc131a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Model Definition (BFC-Net + Modules)\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# ------------------ BCAFM ------------------\n",
    "class BCAFM(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(BCAFM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.query_conv_high = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key_conv_low = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value_conv_low = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "        self.query_conv_low = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key_conv_high = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value_conv_high = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "        self.feedback_conv = nn.Conv2d(channels * 2, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, high_feat, low_feat, feedback=None):\n",
    "        batch_size, C, H, W = low_feat.size()\n",
    "        high_feat_up = F.interpolate(high_feat, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        N = H * W\n",
    "\n",
    "        # Branch 1: High Q, Low K,V\n",
    "        proj_query1 = self.query_conv_high(high_feat_up).view(batch_size, -1, N).permute(0, 2, 1)\n",
    "        proj_key1 = self.key_conv_low(low_feat).view(batch_size, -1, N)\n",
    "        energy1 = torch.bmm(proj_query1, proj_key1)\n",
    "        attention1 = self.softmax(energy1)\n",
    "        proj_value1 = self.value_conv_low(low_feat).view(batch_size, -1, N)\n",
    "        out1 = torch.bmm(proj_value1, attention1.permute(0, 2, 1)).view(batch_size, C, H, W)\n",
    "\n",
    "        # Branch 2: Low Q, High K,V\n",
    "        proj_query2 = self.query_conv_low(low_feat).view(batch_size, -1, N).permute(0, 2, 1)\n",
    "        proj_key2 = self.key_conv_high(high_feat_up).view(batch_size, -1, N)\n",
    "        energy2 = torch.bmm(proj_query2, proj_key2)\n",
    "        attention2 = self.softmax(energy2)\n",
    "        proj_value2 = self.value_conv_high(high_feat_up).view(batch_size, -1, N)\n",
    "        out2 = torch.bmm(proj_value2, attention2.permute(0, 2, 1)).view(batch_size, C, H, W)\n",
    "\n",
    "        # Combine\n",
    "        combined = torch.cat([out1, out2], dim=1)\n",
    "        output = self.feedback_conv(combined)\n",
    "        if feedback is not None:\n",
    "            output = output + self.gamma * feedback\n",
    "        return output\n",
    "\n",
    "# ------------------ AAR ------------------\n",
    "class AAR(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(AAR, self).__init__()\n",
    "        self.fbd_conv = nn.Conv2d(channels, 1, 1)\n",
    "        self.lavs_conv1 = nn.Conv2d(channels, channels // 2, 3, padding=1)\n",
    "        self.lavs_conv2 = nn.Conv2d(channels // 2, channels, 3, padding=1)\n",
    "        self.attention_conv = nn.Conv2d(channels, 1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=-1)  # âœ… Fixed: was missing\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.size()\n",
    "        N = H * W\n",
    "\n",
    "        # FBD\n",
    "        fbd_map = self.sigmoid(self.fbd_conv(x))\n",
    "        foreground_mask = fbd_map\n",
    "        background_mask = 1 - fbd_map\n",
    "\n",
    "        # Global attention\n",
    "        x_flat = x.view(batch_size, C, N)\n",
    "        attention_weights = torch.bmm(x_flat.permute(0, 2, 1), x_flat)\n",
    "        attention_weights = self.softmax(attention_weights)\n",
    "\n",
    "        # Enhanced attention (FG â†” BG)\n",
    "        fg_flat = foreground_mask.view(batch_size, 1, N)\n",
    "        bg_flat = background_mask.view(batch_size, 1, N)\n",
    "        enhanced_attention = fg_flat.permute(0, 2, 1) * attention_weights * bg_flat\n",
    "\n",
    "        # LAVS\n",
    "        local_features = F.relu(self.lavs_conv1(x))\n",
    "        local_features = self.lavs_conv2(local_features)\n",
    "\n",
    "        # Combine\n",
    "        local_flat = local_features.view(batch_size, C, N)\n",
    "        output_flat = torch.bmm(local_flat, enhanced_attention.permute(0, 2, 1))\n",
    "        output = output_flat.view(batch_size, C, H, W)\n",
    "\n",
    "        # Adaptive refinement\n",
    "        adaptive_weights = self.sigmoid(self.attention_conv(x))\n",
    "        output = output * adaptive_weights + x * (1 - adaptive_weights)\n",
    "        return output\n",
    "\n",
    "# ------------------ MSBSF ------------------\n",
    "class MSBSF(nn.Module):\n",
    "    def __init__(self, channels_list):\n",
    "        super(MSBSF, self).__init__()\n",
    "        self.bpc_convs = nn.ModuleList([nn.Conv2d(ch, ch, 3, padding=1) for ch in channels_list])\n",
    "        self.semantic_convs = nn.ModuleList([nn.Conv2d(ch, ch, 3, padding=1) for ch in channels_list])\n",
    "        self.fusion_convs = nn.ModuleList([nn.Conv2d(ch * 2, ch, 1) for ch in channels_list])\n",
    "\n",
    "    def forward(self, features):\n",
    "        enhanced = []\n",
    "        for i, feat in enumerate(features):\n",
    "            boundary = F.relu(self.bpc_convs[i](feat))\n",
    "            semantic = F.relu(self.semantic_convs[i](feat))\n",
    "            fused = torch.cat([boundary, semantic], dim=1)\n",
    "            fused = self.fusion_convs[i](fused)\n",
    "            enhanced.append(fused + feat)\n",
    "        return enhanced\n",
    "\n",
    "# ------------------ BFCNet ------------------\n",
    "class BFCNet(nn.Module):\n",
    "    def __init__(self, backbone='resnet50'):\n",
    "        super(BFCNet, self).__init__()\n",
    "        encoder = resnet50(pretrained=True)\n",
    "        self.encoder_channels = [256, 512, 1024, 2048]\n",
    "\n",
    "        # Encoder\n",
    "        self.layer0 = nn.Sequential(encoder.conv1, encoder.bn1, encoder.relu, encoder.maxpool)\n",
    "        self.layer1 = encoder.layer1\n",
    "        self.layer2 = encoder.layer2\n",
    "        self.layer3 = encoder.layer3\n",
    "        self.layer4 = encoder.layer4\n",
    "\n",
    "        # Modules\n",
    "        self.msbsf = MSBSF(self.encoder_channels)\n",
    "        self.bcafm3 = BCAFM(1024)\n",
    "        self.bcafm2 = BCAFM(512)\n",
    "        self.bcafm1 = BCAFM(256)\n",
    "        self.aar = AAR(2048)\n",
    "\n",
    "        # Decoder convs\n",
    "        self.decoder_conv4 = nn.Conv2d(2048, 1024, 3, padding=1)\n",
    "        self.decoder_conv3 = nn.Conv2d(1024, 512, 3, padding=1)\n",
    "        self.decoder_conv2 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.decoder_conv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "\n",
    "        # Prediction heads\n",
    "        self.pred_conv1 = nn.Conv2d(128, 1, 1)\n",
    "        self.pred_conv2 = nn.Conv2d(256, 1, 1)\n",
    "        self.pred_conv3 = nn.Conv2d(512, 1, 1)\n",
    "        self.final_conv = nn.Conv2d(1024, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.layer0(x)\n",
    "        f1 = self.layer1(x0)\n",
    "        f2 = self.layer2(f1)\n",
    "        f3 = self.layer3(f2)\n",
    "        f4 = self.layer4(f3)\n",
    "        features = [f1, f2, f3, f4]\n",
    "\n",
    "        enhanced = self.msbsf(features)\n",
    "\n",
    "        d4 = self.aar(enhanced[3])\n",
    "        d4 = F.relu(self.decoder_conv4(d4))\n",
    "\n",
    "        # Project f4 â†’ 1024\n",
    "        proj_f4 = F.adaptive_avg_pool2d(enhanced[3], enhanced[2].shape[2:])\n",
    "        proj_f4 = nn.Conv2d(2048, 1024, 1).to(x.device)(proj_f4)\n",
    "\n",
    "        d3 = self.bcafm3(proj_f4, enhanced[2])\n",
    "        d3 = F.relu(self.decoder_conv3(d3))\n",
    "        d3_up = F.interpolate(d3, size=enhanced[1].shape[2:], mode='bilinear')\n",
    "\n",
    "        d2 = self.bcafm2(d3_up, enhanced[1])\n",
    "        d2 = F.relu(self.decoder_conv2(d2))\n",
    "        d2_up = F.interpolate(d2, size=enhanced[0].shape[2:], mode='bilinear')\n",
    "\n",
    "        d1 = self.bcafm1(d2_up, enhanced[0])\n",
    "        d1 = F.relu(self.decoder_conv1(d1))\n",
    "\n",
    "        pred1 = torch.sigmoid(self.pred_conv1(d1))\n",
    "        pred2 = torch.sigmoid(self.pred_conv2(d2))\n",
    "        pred3 = torch.sigmoid(self.pred_conv3(d3))\n",
    "        final_pred = torch.sigmoid(self.final_conv(d4))\n",
    "\n",
    "        return final_pred, [pred1, pred2, pred3]\n",
    "\n",
    "# ------------------ HybridLoss ------------------\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, weights=[0.5, 0.3, 0.2]):\n",
    "        super(HybridLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def iou_loss(self, pred, target):\n",
    "        pred = pred.view(pred.size(0), -1)\n",
    "        target = target.view(target.size(0), -1)\n",
    "        intersection = (pred * target).sum(dim=1)\n",
    "        union = pred.sum(dim=1) + target.sum(dim=1) - intersection\n",
    "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "        return (1 - iou).mean()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        final_pred, side_preds = predictions\n",
    "        targets = targets.float()\n",
    "\n",
    "        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÛŒÚ©Ø³Ø§Ù† Ø¨ÙˆØ¯Ù† Ø§Ø¨Ø¹Ø§Ø¯ final_pred Ùˆ targets\n",
    "        if final_pred.shape != targets.shape:\n",
    "            final_pred = F.interpolate(final_pred, size=targets.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        bce_loss = self.bce_loss(final_pred, targets)\n",
    "        iou_loss = self.iou_loss(final_pred, targets)\n",
    "        total_loss = bce_loss + iou_loss\n",
    "\n",
    "        for i, pred in enumerate(side_preds):\n",
    "            # âœ… Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ·Ø§Ø¨Ù‚ Ø§Ø¨Ø¹Ø§Ø¯\n",
    "            if pred.shape[2:] != targets.shape[2:]:\n",
    "                pred = F.interpolate(pred, size=targets.shape[2:], mode='bilinear', align_corners=False)\n",
    "            side_bce = self.bce_loss(pred, targets)\n",
    "            side_iou = self.iou_loss(pred, targets)\n",
    "            total_loss += self.weights[i] * (side_bce + side_iou)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "print(\"âœ… Model defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGSg_KIEhJFY"
   },
   "source": [
    "### ğŸ“‚ Cell 3: Custom Dataset (Shared Images, Separate GTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:29.701045100Z",
     "start_time": "2025-12-18T11:52:29.591326900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53p4cH7ehJFY",
    "outputId": "c8c21e28-2e8e-4123-e062-5b50b90d88ee",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Dataset\n",
    "class ORSIDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, target_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_names = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        self.mask_names = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "        # Match by name (without extension)\n",
    "        img_ids = {os.path.splitext(f)[0] for f in self.image_names}\n",
    "        mask_ids = {os.path.splitext(f)[0] for f in self.mask_names}\n",
    "        self.common_ids = sorted(img_ids & mask_ids)\n",
    "\n",
    "        assert len(self.common_ids) > 0, f\"No matching files between {image_dir} and {mask_dir}\"\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.common_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            file_id = self.common_ids[idx]\n",
    "            img_file = [f for f in self.image_names if os.path.splitext(f)[0] == file_id][0]\n",
    "            mask_file = [f for f in self.mask_names if os.path.splitext(f)[0] == file_id][0]\n",
    "\n",
    "            image = Image.open(os.path.join(self.image_dir, img_file)).convert(\"RGB\")\n",
    "            mask = Image.open(os.path.join(self.mask_dir, mask_file)).convert(\"L\")\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            if self.target_transform:\n",
    "                mask = self.target_transform(mask)\n",
    "\n",
    "            # ØªØ¨Ø¯ÛŒÙ„ mask Ø¨Ù‡ numpy Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ\n",
    "            mask = np.array(mask).astype(np.float32) / 255.0\n",
    "\n",
    "            # âŒ Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ unsqueeze Ù†ÛŒØ³Øª!\n",
    "            mask = torch.from_numpy(np.array(mask)).float() / 255.0  # ÙÙ‚Ø· Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ\n",
    "\n",
    "            return image, mask  # image: [3, H, W], mask: [1, H, W]\n",
    "\n",
    "            # file_id = self.common_ids[idx]\n",
    "            # # Find full filename (preserve extension)\n",
    "            # img_file = [f for f in self.image_names if os.path.splitext(f)[0] == file_id][0]\n",
    "            # mask_file = [f for f in self.mask_names if os.path.splitext(f)[0] == file_id][0]\n",
    "            #\n",
    "            # image = Image.open(os.path.join(self.image_dir, img_file)).convert(\"RGB\")\n",
    "            # mask = Image.open(os.path.join(self.mask_dir, mask_file)).convert(\"L\")\n",
    "            #\n",
    "            # if self.transform:\n",
    "            #     image = self.transform(image)\n",
    "            # if self.target_transform:\n",
    "            #     mask = self.target_transform(mask)\n",
    "            #\n",
    "            # mask = torch.from_numpy(np.array(mask)).float() / 255.0\n",
    "            # mask = mask.unsqueeze(0)\n",
    "            # return image, mask\n",
    "        except Exception as e:\n",
    "            file_id = self.common_ids[idx]\n",
    "            print(f\"âŒ Error loading {file_id}: {e}\")\n",
    "            raise e\n",
    "\n",
    "print(\"âœ… Dataset class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFfOVgNjhJFZ"
   },
   "source": [
    "### âš™ï¸ Cell 4: Training Configuration (Ù…Ø·Ø§Ø¨Ù‚ Ù…Ù‚Ø§Ù„Ù‡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:29.825589700Z",
     "start_time": "2025-12-18T11:52:29.716268800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "PNjbJl7KhJFZ",
    "outputId": "adb48f21-a73d-4d6d-ca37-fd25f1864149",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Config\n",
    "# INPUT_SIZE = 256  # âš ï¸ Ú©Ø§Ù‡Ø´ Ø§Ø² 352 Ø¨Ù‡ 256\n",
    "# BATCH_SIZE = 4    # âš ï¸ Ú©Ø§Ù‡Ø´ Ø§Ø² 8 Ø¨Ù‡ 1\n",
    "INPUT_SIZE = 352\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize((INPUT_SIZE, INPUT_SIZE), interpolation=Image.NEAREST),\n",
    "])\n",
    "\n",
    "# Base image path (shared)\n",
    "# base_path = '/content/drive/MyDrive/BFC_NET/data'\n",
    "base_path = './data'\n",
    "IMAGE_TRAIN_DIR = f\"{base_path}/ORSSD/Image-train\"\n",
    "# IMAGE_TRAIN_DIR = \"/kaggle/input/orssd-dataset/data/ORSSD/Image-train\"\n",
    "\n",
    "# Dataset-specific GT paths\n",
    "DATASETS = {\n",
    "    \"ORSSD\": f\"{base_path}/ORSSD/GT-train\",\n",
    "    # \"ORSSD\": \"/kaggle/input/orssd-dataset/data/ORSSD/GT-train\",\n",
    "    # \"EORSSD\": \"./data/EORSSD/GT-train\",\n",
    "    # \"ORSI-4199\": \"./data/ORSI4199/GT-train\"\n",
    "}\n",
    "assert os.path.exists(IMAGE_TRAIN_DIR), f\"Image dir not found: {IMAGE_TRAIN_DIR}\"\n",
    "assert os.path.exists(DATASETS.get('ORSSD')), f\"GT dir not found: {DATASETS.get('ORSSD')}\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9YbHDaMhJFa"
   },
   "source": [
    "### ğŸš€ Cell 5: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:29.890854400Z",
     "start_time": "2025-12-18T11:52:29.825589700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiReU4H0hJFa",
    "outputId": "08b09063-3780-4162-fe7e-f91a31193e13",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Training Loop\n",
    "# def train_on_dataset(dataset_name, gt_dir):\n",
    "#     print(f\"\\nğŸš€ Training on {dataset_name}...\")\n",
    "#     torch.cuda.empty_cache()  # Ù¾Ø§Ú©â€ŒÚ©Ø±Ø¯Ù† Ø­Ø§ÙØ¸Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ø´Ø±ÙˆØ¹\n",
    "#     dataset = ORSIDataset(IMAGE_TRAIN_DIR, gt_dir, train_transform, target_transform)\n",
    "#\n",
    "#     # print(f\"Input size: {INPUT_SIZE}x{INPUT_SIZE}\")\n",
    "#     # print(f\"Batch size: {BATCH_SIZE}\")\n",
    "#     # print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "#\n",
    "#     # dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "#     dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "#\n",
    "#     model = BFCNet(backbone='resnet50').to(device)\n",
    "#     criterion = HybridLoss(weights=[0.5, 0.3, 0.2])\n",
    "#     # optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "#\n",
    "#     scheduler = StepLR(optimizer, step_size=20, gamma=0.1)  # Ù‡Ø± 20 Ø§Ù¾ÙˆÚ© Ã—0.1\n",
    "#\n",
    "#     model.train()\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         torch.cuda.empty_cache()  # â† Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯\n",
    "#         total_loss = 0.0\n",
    "#         pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "#         for images, masks in pbar:\n",
    "#             images, masks = images.to(device), masks.to(device)\n",
    "#\n",
    "#             optimizer.zero_grad()\n",
    "#             final_pred, side_preds = model(images)\n",
    "#             loss = criterion((final_pred, side_preds), masks)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#\n",
    "#             total_loss += loss.item()\n",
    "#             pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "#\n",
    "#         avg_loss = total_loss / len(dataloader)\n",
    "#         print(f\"âœ… Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
    "#\n",
    "#         scheduler.step()  # âœ… Ú©Ø§Ù‡Ø´ Ø®ÙˆØ¯Ú©Ø§Ø± Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ\n",
    "#\n",
    "#         # Save checkpoint\n",
    "#         if (epoch + 1) % 20 == 0:\n",
    "#             torch.save(model.state_dict(), f\"BFCNet_{dataset_name}_epoch{epoch+1}.pth\")\n",
    "#\n",
    "#         # torch.cuda.empty_cache()\n",
    "#\n",
    "#     torch.save(model.state_dict(), f\"BFCNet_{dataset_name}_final.pth\")\n",
    "#     print(f\"ğŸ’¾ Model saved for {dataset_name}!\")\n",
    "#\n",
    "# # Train on all three datasets\n",
    "# for name, gt_path in DATASETS.items():\n",
    "#     train_on_dataset(name, gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:30.013442900Z",
     "start_time": "2025-12-18T11:52:29.903287800Z"
    },
    "id": "umgmrlRI3gzt"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Evaluation Metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_Mae(pred, gt):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    return np.mean(np.abs(pred - gt))\n",
    "\n",
    "def calculate_S_measure(pred, gt, alpha=0.5):\n",
    "    \"\"\"Structure-measure from Fan et al. (ICCV 2017)\"\"\"\n",
    "    y = np.array(gt).astype(np.float32)\n",
    "    pred = np.array(pred).astype(np.float32)\n",
    "\n",
    "    if np.mean(y) == 0:  # if ground truth is all black\n",
    "        x = 1 - pred.mean()\n",
    "        Q = 1 - x.mean()\n",
    "    elif np.mean(y) == 1:  # if ground truth is all white\n",
    "        x = pred.mean()\n",
    "        Q = x.mean()\n",
    "    else:\n",
    "        # Region-aware similarity\n",
    "        y_fg = y - y.min()\n",
    "        y_fg /= y_fg.max()\n",
    "        y_bg = 1 - y\n",
    "        y_bg = y_bg - y_bg.min()\n",
    "        y_bg /= y_bg.max()\n",
    "\n",
    "        w_fg = np.sum(y_fg)\n",
    "        w_bg = np.sum(y_bg)\n",
    "        w = w_fg + w_bg + np.finfo(float).eps\n",
    "\n",
    "        y_fg = y_fg.reshape(-1)\n",
    "        y_bg = y_bg.reshape(-1)\n",
    "        pred = pred.reshape(-1)\n",
    "        Q_fg = np.mean((1 - y_fg) * pred)\n",
    "        Q_bg = np.mean((1 - y_bg) * (1 - pred))\n",
    "        Q = alpha * Q_fg + (1 - alpha) * Q_bg\n",
    "    return Q\n",
    "\n",
    "def calculate_fmeasure(pred, gt, beta2=0.3):\n",
    "    \"\"\"Weighted F-measure\"\"\"\n",
    "    prec, recall = [], []\n",
    "    for th in np.linspace(0, 1, 256):\n",
    "        bi_pred = (pred >= th).astype(np.float32)\n",
    "        tp = np.sum(bi_pred * gt)\n",
    "        prec.append(tp / (np.sum(bi_pred) + 1e-8))\n",
    "        recall.append(tp / (np.sum(gt) + 1e-8))\n",
    "    prec, recall = np.array(prec), np.array(recall)\n",
    "    f_score = (1 + beta2) * prec * recall / (beta2 * prec + recall + 1e-8)\n",
    "    return f_score.max()\n",
    "\n",
    "def calculate_iou(pred, gt):\n",
    "    \"\"\"Intersection over Union\"\"\"\n",
    "    inter = np.sum(pred * gt)\n",
    "    union = np.sum(pred) + np.sum(gt) - inter\n",
    "    return (inter + 1e-8) / (union + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:30.107366Z",
     "start_time": "2025-12-18T11:52:30.059947700Z"
    },
    "id": "yjdM_H2A3mAx"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, dataset_name):\n",
    "    model.eval()\n",
    "    mae_list, fm_list, sm_list, iou_list = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_loader, desc=f\"Evaluating {dataset_name}\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)  # [B, 1, H, W]\n",
    "\n",
    "            # âœ… Ù…Ø¯Ù„ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø¨Ø§ sigmoid ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ â€” Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ sigmoid Ù…Ø¬Ø¯Ø¯ Ù†ÛŒØ³Øª\n",
    "            final_pred, _ = model(images)  # final_pred: [B, 1, H, W] Ø¯Ø± Ø¨Ø§Ø²Ù‡ [0, 1]\n",
    "\n",
    "            # âœ… ØªØ·Ø¨ÛŒÙ‚ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙÙ‚Ø· Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯\n",
    "            if final_pred.shape[2:] != masks.shape[2:]:\n",
    "                final_pred = F.interpolate(final_pred, size=masks.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # âœ… ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ numpy Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§\n",
    "            pred_np = final_pred.cpu().numpy().squeeze(1)  # [B, H, W]\n",
    "            mask_np = masks.cpu().numpy().squeeze(1)       # [B, H, W]\n",
    "\n",
    "            for i in range(pred_np.shape[0]):\n",
    "                pred_i = pred_np[i]\n",
    "                gt_i = mask_np[i]\n",
    "\n",
    "                # âœ… Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø§ÛŒØ¯ Ø¯Ø± [0, 1] Ø¨Ø§Ø´Ù†Ø¯\n",
    "                assert pred_i.min() >= 0 and pred_i.max() <= 1, \"Prediction not in [0,1]!\"\n",
    "\n",
    "                mae = calculate_Mae(pred_i, gt_i)\n",
    "                fm = calculate_fmeasure(pred_i, gt_i)\n",
    "                sm = calculate_S_measure(pred_i, gt_i)\n",
    "                iou = calculate_iou(pred_i, gt_i)\n",
    "\n",
    "                mae_list.append(mae)\n",
    "                fm_list.append(fm)\n",
    "                sm_list.append(sm)\n",
    "                iou_list.append(iou)\n",
    "\n",
    "    avg_mae = np.mean(mae_list)\n",
    "    avg_fm = np.mean(fm_list)\n",
    "    avg_sm = np.mean(sm_list)\n",
    "    avg_iou = np.mean(iou_list)\n",
    "\n",
    "    print(f\"\\nâœ… Results on {dataset_name}:\")\n",
    "    print(f\"  MAE â†“: {avg_mae:.4f}\")\n",
    "    print(f\"  F-measure â†‘: {avg_fm:.4f}\")\n",
    "    print(f\"  S-measure â†‘: {avg_sm:.4f}\")\n",
    "    print(f\"  IoU â†‘: {avg_iou:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'mae': avg_mae,\n",
    "        'fmeasure': avg_fm,\n",
    "        'smeasure': avg_sm,\n",
    "        'iou': avg_iou,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:30.184699500Z",
     "start_time": "2025-12-18T11:52:30.107366Z"
    },
    "id": "C9iLe2263ncp"
   },
   "outputs": [],
   "source": [
    "# Cell 8: Plot F-measure and PR curves (Optional vs SOTA)\n",
    "def plot_curves(all_results, dataset_name):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # F-measure curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # Ø¯Ø± Ø¹Ù…Ù„ØŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ SOTAØŒ Ø¨Ø§ÛŒØ¯ Ù†ØªØ§ÛŒØ¬ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯\n",
    "    # Ø§ÛŒÙ†Ø¬Ø§ ÙÙ‚Ø· F-measure max Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…\n",
    "    plt.bar(['Ours'], [all_results['fmeasure']], color='skyblue')\n",
    "    plt.title(f'F-measure on {dataset_name}')\n",
    "    plt.ylabel('F-measure (max)')\n",
    "\n",
    "    # Ù†Ù…ÙˆØ¯Ø§Ø± MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(['Ours'], [all_results['mae']], color='salmon')\n",
    "    plt.title(f'MAE on {dataset_name}')\n",
    "    plt.ylabel('MAE â†“')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results_{dataset_name}.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:30.247419800Z",
     "start_time": "2025-12-18T11:52:30.184699500Z"
    },
    "id": "Y1ax-09B3su1"
   },
   "outputs": [],
   "source": [
    "# Cell 9: Save qualitative results\n",
    "def save_qualitative_results(model, test_loader, device, dataset_name, num_samples=5):\n",
    "    model.eval()\n",
    "    os.makedirs(f\"results/{dataset_name}\", exist_ok=True)\n",
    "\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "            images = images.to(device)\n",
    "            final_pred, _ = model(images)\n",
    "            pred = final_pred[0].cpu().numpy().squeeze()\n",
    "            gt = masks[0].numpy().squeeze()\n",
    "            img = images[0].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "            # Ø°Ø®ÛŒØ±Ù‡ ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒØŒ GT Ùˆ Ù†ØªÛŒØ¬Ù‡\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 3, 1); plt.imshow(img); plt.title(\"Input\"); plt.axis('off')\n",
    "            plt.subplot(1, 3, 2); plt.imshow(gt, cmap='gray'); plt.title(\"GT\"); plt.axis('off')\n",
    "            plt.subplot(1, 3, 3); plt.imshow(pred, cmap='gray'); plt.title(\"Prediction\"); plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"results/{dataset_name}/sample_{count+1}.png\")\n",
    "            plt.close()\n",
    "\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:52:30.295030900Z",
     "start_time": "2025-12-18T11:52:30.247419800Z"
    },
    "id": "pQpclnBP3wm2"
   },
   "outputs": [],
   "source": [
    "# Cell 10: Run evaluation after training\n",
    "def train_and_evaluate(dataset_name, gt_train, gt_test, img_train, img_test):\n",
    "    # Ø¢Ù…ÙˆØ²Ø´\n",
    "    # train_on_dataset(dataset_name, gt_train)\n",
    "\n",
    "    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡\n",
    "    model = BFCNet(backbone='resnet50').to(device)\n",
    "    model.load_state_dict(torch.load(f\"BFCNet_{dataset_name}_final.pth\"))\n",
    "\n",
    "    # Ø§ÛŒØ¬Ø§Ø¯ DataLoader Ø¨Ø±Ø§ÛŒ ØªØ³Øª\n",
    "    # test_dataset = ORSIDataset(img_test, gt_test, transform=None, target_transform=None)\n",
    "\n",
    "    # âœ… ØªØ±Ø§Ù†Ø³ÙÙˆØ±Ù… ØªØ³Øª (Ø¶Ø±ÙˆØ±ÛŒ!)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((352, 352)),  # â† Ø¶Ø±ÙˆØ±ÛŒ!\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.Resize((352, 352), interpolation=Image.NEAREST),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    test_dataset = ORSIDataset(\n",
    "        img_test, gt_test,\n",
    "        transform=test_transform,\n",
    "        target_transform=target_transform\n",
    "    )\n",
    "\n",
    "    img, mask = test_dataset[0]\n",
    "    plt.subplot(1,2,1); plt.imshow(img.permute(1,2,0)); plt.title(\"Input\")\n",
    "    plt.subplot(1,2,2); plt.imshow(mask.squeeze(), cmap='gray'); plt.title(\"GT\")\n",
    "    plt.show()\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
    "    results = evaluate_model(model, test_loader, device, dataset_name)\n",
    "    plot_curves(results, dataset_name)\n",
    "    save_qualitative_results(model, test_loader, device, dataset_name)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T11:53:11.997479900Z",
     "start_time": "2025-12-18T11:52:30.309764600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "wzQZJM2F50hK",
    "outputId": "791e31a5-5b4a-43e4-bea4-1d1767559520"
   },
   "outputs": [],
   "source": [
    "# ØªØºÛŒÛŒØ± Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ ØªØ³Øª\n",
    "base_path = \"./data\"\n",
    "IMAGE_TRAIN_DIR = f\"{base_path}/ORSSD/Image-train\"\n",
    "# Dataset-specific GT paths\n",
    "DATASETS = {\n",
    "    \"ORSSD\": f\"{base_path}/ORSSD/GT-train\",\n",
    "    # \"ORSSD\": \"/kaggle/input/orssd-dataset/data/ORSSD/GT-train\",\n",
    "    # \"EORSSD\": \"./data/EORSSD/GT-train\",\n",
    "    # \"ORSI-4199\": \"./data/ORSI4199/GT-train\"\n",
    "}\n",
    "DATASETS_TEST = {\n",
    "    \"ORSSD\": (f\"{base_path}/ORSSD/Image-test\", f\"{base_path}/ORSSD/GT-test\"),\n",
    "    # \"EORSSD\": (\"./data/EORSSD/Image-test\", \"./data/EORSSD/GT-test\"),\n",
    "    # \"ORSI-4199\": (\"./data/ORSI4199/Image-test\", \"./data/ORSI4199/GT-test\")\n",
    "}\n",
    "# ØªØ³Øª ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ\n",
    "train_dataset = ORSIDataset(IMAGE_TRAIN_DIR, DATASETS[\"ORSSD\"],\n",
    "                            transform=train_transform,\n",
    "                            target_transform=target_transform)\n",
    "img, mask = train_dataset[0]\n",
    "\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(\"Mask shape:\", mask.shape)\n",
    "print(\"Mask unique values:\", torch.unique(mask))\n",
    "print(\"Mask mean:\", mask.mean().item())\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ± Ùˆ Ù…Ø§Ø³Ú©\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1); plt.imshow(img.permute(1, 2, 0)); plt.title(\"Input\")\n",
    "plt.subplot(1, 3, 2); plt.imshow(mask.squeeze(), cmap='gray'); plt.title(\"GT\")\n",
    "plt.subplot(1, 3, 3); plt.hist(mask.numpy().flatten(), bins=50); plt.title(\"Mask Histogram\")\n",
    "plt.show()\n",
    "# Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ + Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
    "results_all = []\n",
    "for name in [\"ORSSD\"]: #, \"EORSSD\", \"ORSI-4199\"\n",
    "    img_train = IMAGE_TRAIN_DIR\n",
    "    gt_train = DATASETS[name]\n",
    "    img_test, gt_test = DATASETS_TEST[name]\n",
    "    res = train_and_evaluate(name, gt_train, gt_test, img_train, img_test)\n",
    "    results_all.append(res)\n",
    "\n",
    "# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ CSV (Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ Ù…Ù‚Ø§Ù„Ù‡)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results_all)\n",
    "df.to_csv(\"results_summary.csv\", index=False)\n",
    "print(\"\\nğŸ“Š Results saved to results_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8991859,
     "sourceId": 14115322,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
