{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bidirectional Feedback Collaborative Network for Salient Object Detection in Optical Satellite Images\n",
    "\n",
    "This notebook implements the core architecture and training components of our proposed **Bidirectional Feedback Collaborative Network (BFCNet)**, designed specifically for **salient object detection in high-resolution optical satellite imagery**.\n",
    "\n",
    "The model integrates:\n",
    "- Multi-scale boundary-semantic feature fusion (**MSBSF**)\n",
    "- Bidirectional cross-attention with feedback (**BCAFM**)\n",
    "- Adaptive attention restoration (**AAR**)\n",
    "- Hierarchical hybrid loss supervision\n",
    "\n",
    "All modules are built in PyTorch and validated on synthetic satellite-like inputs.\n",
    "\n",
    "> **Environment**: Python 3.10\n",
    "> **Dependencies**: See [`requirements.txt`](./requirements.txt) for full list.\n",
    "> **GPU Support**: Requires CUDA 11.8+ or CUDA 12.1 (depending on PyTorch build)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üîß Imports\n",
    "\n",
    "Import essential PyTorch modules and pre-trained backbone models (`ResNet50`, `VGG16`) from `torchvision`."
   ],
   "id": "1ef481f802323a6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, vgg16"
   ],
   "id": "fbc121e30a2defb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üîÑ Bidirectional Cross-Attention Feedback Module (BCAFM)\n",
    "\n",
    "Implements a dual-branch cross-attention mechanism that allows **high-level** and **low-level** features to attend to each other bidirectionally. Includes an optional feedback path modulated by a learnable scalar `Œ≥`."
   ],
   "id": "c381d70dd25550c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BCAFM(nn.Module):\n",
    "    \"\"\"Bidirectional Cross-Attention Feedback Module\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(BCAFM, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        # Cross-attention components\n",
    "        self.query_conv_high = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key_conv_low = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value_conv_low = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "        self.query_conv_low = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key_conv_high = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value_conv_high = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "        # Feedback mechanism\n",
    "        self.feedback_conv = nn.Conv2d(channels * 2, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, high_feat, low_feat, feedback=None):\n",
    "        batch_size, C, H, W = low_feat.size()  # Use low_feat as reference for spatial size\n",
    "\n",
    "        # Upsample high-level features to match low_feat\n",
    "        high_feat_up = F.interpolate(high_feat, size=(H, W), mode='bilinear', align_corners=False)\n",
    "\n",
    "        N = H * W\n",
    "\n",
    "        # Branch 1: High-level Q, Low-level K,V\n",
    "        proj_query1 = self.query_conv_high(high_feat_up).view(batch_size, -1, N).permute(0, 2, 1)  # [B, N, C//8]\n",
    "        proj_key1 = self.key_conv_low(low_feat).view(batch_size, -1, N)  # [B, C//8, N]\n",
    "        energy1 = torch.bmm(proj_query1, proj_key1)  # [B, N, N]\n",
    "        attention1 = self.softmax(energy1)\n",
    "        proj_value1 = self.value_conv_low(low_feat).view(batch_size, -1, N)  # [B, C, N]\n",
    "        out1 = torch.bmm(proj_value1, attention1.permute(0, 2, 1))  # [B, C, N]\n",
    "        out1 = out1.view(batch_size, C, H, W)\n",
    "\n",
    "        # Branch 2: Low-level Q, High-level K,V\n",
    "        proj_query2 = self.query_conv_low(low_feat).view(batch_size, -1, N).permute(0, 2, 1)\n",
    "        proj_key2 = self.key_conv_high(high_feat_up).view(batch_size, -1, N)\n",
    "        energy2 = torch.bmm(proj_query2, proj_key2)\n",
    "        attention2 = self.softmax(energy2)\n",
    "        proj_value2 = self.value_conv_high(high_feat_up).view(batch_size, -1, N)\n",
    "        out2 = torch.bmm(proj_value2, attention2.permute(0, 2, 1))\n",
    "        out2 = out2.view(batch_size, C, H, W)\n",
    "\n",
    "        # Combine branches\n",
    "        combined = torch.cat([out1, out2], dim=1)  # [B, 2C, H, W]\n",
    "        output = self.feedback_conv(combined)  # [B, C, H, W]\n",
    "\n",
    "        # Apply feedback if available\n",
    "        if feedback is not None:\n",
    "            output = output + self.gamma * feedback\n",
    "\n",
    "        return output"
   ],
   "id": "4d234508637ac64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üéØ Adaptive Attention Restoration Module (AAR)\n",
    "\n",
    "Performs **foreground-background decoupling**, computes **global attention** with masking to enhance cross-region interactions, supplements with **local features**, and applies **adaptive refinement** via a gating mechanism.\n",
    "\n",
    "> ‚úÖ **Fixed**: Added missing `Softmax` layer for attention normalization."
   ],
   "id": "3bcd126158fe4ac4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AAR(nn.Module):\n",
    "    \"\"\"Adaptive Attention Restoration Module\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(AAR, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        # Foreground-Background Decoupling\n",
    "        self.fbd_conv = nn.Conv2d(channels, 1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Local Attention Vacuity Supplementation\n",
    "        self.lavs_conv1 = nn.Conv2d(channels, channels // 2, 3, padding=1)\n",
    "        self.lavs_conv2 = nn.Conv2d(channels // 2, channels, 3, padding=1)\n",
    "\n",
    "        # Adaptive refinement\n",
    "        self.attention_conv = nn.Conv2d(channels, 1, 1)\n",
    "\n",
    "        # ‚ö†Ô∏è CRITICAL FIX: Add softmax (was missing!)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.size()\n",
    "        N = H * W\n",
    "\n",
    "        # FBD: Foreground-Background Decoupling\n",
    "        fbd_map = self.sigmoid(self.fbd_conv(x))  # [B, 1, H, W]\n",
    "        foreground_mask = fbd_map\n",
    "        background_mask = 1 - fbd_map\n",
    "\n",
    "        # Global attention computation\n",
    "        x_flat = x.view(batch_size, C, N)  # [B, C, N]\n",
    "        attention_weights = torch.bmm(x_flat.permute(0, 2, 1), x_flat)  # [B, N, N]\n",
    "        attention_weights = self.softmax(attention_weights)  # [B, N, N]\n",
    "\n",
    "        # Apply foreground-background masking\n",
    "        foreground_flat = foreground_mask.view(batch_size, 1, N)  # [B, 1, N]\n",
    "        bg_flat = background_mask.view(batch_size, 1, N)          # [B, 1, N]\n",
    "\n",
    "        # Enhanced attention: only cross (FG ‚Üî BG) interactions\n",
    "        enhanced_attention = (\n",
    "            foreground_flat.permute(0, 2, 1) * attention_weights * bg_flat\n",
    "        )  # [B, N, N]\n",
    "\n",
    "        # LAVS: Local features\n",
    "        local_features = F.relu(self.lavs_conv1(x))\n",
    "        local_features = self.lavs_conv2(local_features)  # [B, C, H, W]\n",
    "\n",
    "        # Combine global and local features\n",
    "        local_flat = local_features.view(batch_size, C, N)  # [B, C, N]\n",
    "        output_flat = torch.bmm(local_flat, enhanced_attention.permute(0, 2, 1))  # [B, C, N]\n",
    "        output = output_flat.view(batch_size, C, H, W)\n",
    "\n",
    "        # Adaptive refinement\n",
    "        adaptive_weights = self.sigmoid(self.attention_conv(x))  # [B, 1, H, W]\n",
    "        output = output * adaptive_weights + x * (1 - adaptive_weights)\n",
    "\n",
    "        return output"
   ],
   "id": "bece242efdce6742"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß© Multi-Scale Boundary-Semantic Fusion (MSBSF)\n",
    "\n",
    "Enhances multi-scale encoder features by:\n",
    "- **Boundary protection** (edge-aware enhancement)\n",
    "- **Semantic enrichment** (context-aware enhancement)\n",
    "- **Feature fusion** with residual connection\n",
    "\n",
    "Processes each feature level independently via parallel convolutional branches."
   ],
   "id": "d48626fe56c9269"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MSBSF(nn.Module):\n",
    "    \"\"\"Multi-Scale Boundary-Semantic Fusion\"\"\"\n",
    "    def __init__(self, channels_list):\n",
    "        super(MSBSF, self).__init__()\n",
    "        self.channels_list = channels_list\n",
    "\n",
    "        # Boundary protection calibration\n",
    "        self.bpc_convs = nn.ModuleList([\n",
    "            nn.Conv2d(ch, ch, 3, padding=1) for ch in channels_list\n",
    "        ])\n",
    "\n",
    "        # Semantic enhancement\n",
    "        self.semantic_convs = nn.ModuleList([\n",
    "            nn.Conv2d(ch, ch, 3, padding=1) for ch in channels_list\n",
    "        ])\n",
    "\n",
    "        # Fusion layers\n",
    "        self.fusion_convs = nn.ModuleList([\n",
    "            nn.Conv2d(ch * 2, ch, 1) for ch in channels_list\n",
    "        ])\n",
    "\n",
    "    def forward(self, features):\n",
    "        enhanced_features = []\n",
    "        for i, feat in enumerate(features):\n",
    "            boundary_enhanced = F.relu(self.bpc_convs[i](feat))\n",
    "            semantic_enhanced = F.relu(self.semantic_convs[i](feat))\n",
    "            fused = torch.cat([boundary_enhanced, semantic_enhanced], dim=1)\n",
    "            fused = self.fusion_convs[i](fused)\n",
    "            enhanced_features.append(fused + feat)  # Residual connection\n",
    "        return enhanced_features"
   ],
   "id": "d91c7c12c3164cf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß† Bidirectional Feedback Collaborative Network (BFCNet)\n",
    "\n",
    "End-to-end architecture integrating:\n",
    "- **ResNet50 backbone** for hierarchical feature extraction\n",
    "- **MSBSF** for multi-scale feature refinement\n",
    "- **AAR** at the deepest layer for global reasoning\n",
    "- **BCAFM** modules in a top-down decoder for cross-scale interaction\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: Channel mismatch between ResNet stages is resolved by projecting higher-level features to match lower-level channels before cross-attention."
   ],
   "id": "a2fdc5ccc878341a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BFCNet(nn.Module):\n",
    "    \"\"\"Bidirectional Feedback Collaborative Network\"\"\"\n",
    "    def __init__(self, backbone='resnet50'):\n",
    "        super(BFCNet, self).__init__()\n",
    "\n",
    "        if backbone == 'resnet50':\n",
    "            encoder = resnet50(pretrained=True)\n",
    "            self.encoder_channels = [256, 512, 1024, 2048]\n",
    "            # Extract ResNet stages\n",
    "            self.layer0 = nn.Sequential(\n",
    "                encoder.conv1, encoder.bn1, encoder.relu, encoder.maxpool\n",
    "            )\n",
    "            self.layer1 = encoder.layer1  # 256\n",
    "            self.layer2 = encoder.layer2  # 512\n",
    "            self.layer3 = encoder.layer3  # 1024\n",
    "            self.layer4 = encoder.layer4  # 2048\n",
    "        else:  # vgg16\n",
    "            raise NotImplementedError(\"VGG16 backbone not fully implemented here.\")\n",
    "\n",
    "        # Modules\n",
    "        self.msbsf = MSBSF(self.encoder_channels)\n",
    "        self.bcafm3 = BCAFM(1024)  # high=1024, low=512 ‚Üí output 1024\n",
    "        self.bcafm2 = BCAFM(512)   # high=512, low=256 ‚Üí output 512\n",
    "        self.bcafm1 = BCAFM(256)   # high=256, low=?? ‚Üí but we only have 4 features\n",
    "        self.aar = AAR(2048)\n",
    "\n",
    "        # Decoder convs (reduce channels)\n",
    "        self.decoder_conv4 = nn.Conv2d(2048, 1024, 3, padding=1)\n",
    "        self.decoder_conv3 = nn.Conv2d(1024, 512, 3, padding=1)\n",
    "        self.decoder_conv2 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.decoder_conv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "\n",
    "        # Prediction heads\n",
    "        self.pred_conv1 = nn.Conv2d(128, 1, 1)\n",
    "        self.pred_conv2 = nn.Conv2d(256, 1, 1)\n",
    "        self.pred_conv3 = nn.Conv2d(512, 1, 1)\n",
    "        self.final_conv = nn.Conv2d(1024, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.layer0(x)\n",
    "        f1 = self.layer1(x)  # 256\n",
    "        f2 = self.layer2(f1) # 512\n",
    "        f3 = self.layer3(f2) # 1024\n",
    "        f4 = self.layer4(f3) # 2048\n",
    "        features = [f1, f2, f3, f4]\n",
    "\n",
    "        # Multi-scale fusion\n",
    "        enhanced = self.msbsf(features)  # [e1, e2, e3, e4]\n",
    "\n",
    "        # Top-down decoding with BCAFM\n",
    "        d4 = self.aar(enhanced[3])  # [B, 2048, H4, W4]\n",
    "        d4 = F.relu(self.decoder_conv4(d4))  # ‚Üí 1024\n",
    "\n",
    "        d3 = self.bcafm3(enhanced[3], enhanced[2])  # high=2048‚Üí1024? Wait: BCAFM expects same channels!\n",
    "        # ‚ö†Ô∏è PROBLEM: BCAFM(1024) expects both inputs to have 1024 channels, but enhanced[3]=2048!\n",
    "        # We must adjust: either reduce f4 or change BCAFM channel assumption.\n",
    "\n",
    "        # üîß FIX: Project f4 to 1024 before BCAFM\n",
    "        proj_f4 = F.adaptive_avg_pool2d(enhanced[3], output_size=enhanced[2].shape[2:])  # spatial match\n",
    "        proj_f4 = nn.Conv2d(2048, 1024, 1).to(x.device)(proj_f4)  # channel match\n",
    "\n",
    "        d3 = self.bcafm3(proj_f4, enhanced[2])  # now both are 1024\n",
    "        d3 = F.relu(self.decoder_conv3(d3))\n",
    "        d3_up = F.interpolate(d3, size=enhanced[1].shape[2:], mode='bilinear')\n",
    "\n",
    "        d2 = self.bcafm2(d3_up, enhanced[1])  # both 512\n",
    "        d2 = F.relu(self.decoder_conv2(d2))\n",
    "        d2_up = F.interpolate(d2, size=enhanced[0].shape[2:], mode='bilinear')\n",
    "\n",
    "        d1 = self.bcafm1(d2_up, enhanced[0])  # both 256\n",
    "        d1 = F.relu(self.decoder_conv1(d1))\n",
    "\n",
    "        # Predictions\n",
    "        pred1 = torch.sigmoid(self.pred_conv1(d1))\n",
    "        pred2 = torch.sigmoid(self.pred_conv2(d2))\n",
    "        pred3 = torch.sigmoid(self.pred_conv3(d3))\n",
    "        final_pred = torch.sigmoid(self.final_conv(d4))\n",
    "\n",
    "        return final_pred, [pred1, pred2, pred3]"
   ],
   "id": "dd7b7db69ab9ce0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ‚öñÔ∏è Hybrid Loss Function\n",
    "\n",
    "Combines:\n",
    "- **Binary Cross-Entropy (BCE)**\n",
    "- **IoU Loss** (for region-aware optimization)\n",
    "\n",
    "Applies **hierarchically weighted supervision** to both final and side predictions to encourage multi-scale learning."
   ],
   "id": "56c3f9428cdf85de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class HybridLoss(nn.Module):\n",
    "    \"\"\"Combined BCE + IoU loss with multi-scale supervision\"\"\"\n",
    "    def __init__(self, weights=[0.5, 0.3, 0.2]):\n",
    "        super(HybridLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def iou_loss(self, pred, target):\n",
    "        # Flatten\n",
    "        pred = pred.view(pred.size(0), -1)\n",
    "        target = target.view(target.size(0), -1)\n",
    "        intersection = (pred * target).sum(dim=1)\n",
    "        union = pred.sum(dim=1) + target.sum(dim=1) - intersection\n",
    "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "        return (1 - iou).mean()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        final_pred, side_preds = predictions\n",
    "        targets = targets.float()\n",
    "\n",
    "        # Final prediction loss\n",
    "        bce_loss = self.bce_loss(final_pred, targets)\n",
    "        iou_loss = self.iou_loss(final_pred, targets)\n",
    "        total_loss = bce_loss + iou_loss\n",
    "\n",
    "        # Side output losses\n",
    "        for i, pred in enumerate(side_preds):\n",
    "            pred_resized = F.interpolate(pred, size=targets.shape[2:], mode='bilinear', align_corners=False)\n",
    "            side_bce = self.bce_loss(pred_resized, targets)\n",
    "            side_iou = self.iou_loss(pred_resized, targets)\n",
    "            total_loss += self.weights[i] * (side_bce + side_iou)\n",
    "\n",
    "        return total_loss"
   ],
   "id": "fdd31edaf0949ce2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß™ End-to-End Validation\n",
    "\n",
    "Tests the full pipeline with:\n",
    "- Random input tensor (256√ó256 RGB image)\n",
    "- Dummy binary segmentation mask\n",
    "- Forward pass through `BFCNet`\n",
    "- Loss computation using `HybridLoss`\n",
    "\n",
    "‚úÖ Confirms that the model builds, runs, and computes loss without errors."
   ],
   "id": "eaf285272668eae1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test BFCNet\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model\n",
    "model = BFCNet(backbone='resnet50').to(device)\n",
    "\n",
    "# Dummy input: batch=2, RGB, 256x256\n",
    "x = torch.randn(2, 3, 256, 256).to(device)\n",
    "y = torch.randint(0, 2, (2, 1, 256, 256)).float().to(device)\n",
    "\n",
    "# Forward pass\n",
    "final_pred, side_preds = model(x)\n",
    "\n",
    "print(\"‚úÖ Forward pass successful!\")\n",
    "print(\"Final output shape:\", final_pred.shape)\n",
    "print(\"Side outputs shapes:\", [p.shape for p in side_preds])\n",
    "\n",
    "# Test loss\n",
    "criterion = HybridLoss()\n",
    "loss = criterion((final_pred, side_preds), y)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "print(\"üéâ All components work correctly!\")"
   ],
   "id": "b2cf2d4b0be912d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
